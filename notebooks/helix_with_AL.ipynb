{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "helix with AL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import the necessary modules**"
      ],
      "metadata": {
        "id": "a5bCeggg0Ly1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "metadata": {
        "id": "tVZv_3DF0G5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the surface as a neural net**"
      ],
      "metadata": {
        "id": "JOevpnmqrXJR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1xKUHsSx-aH"
      },
      "outputs": [],
      "source": [
        "class LSTMForgetBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_nodes, dtype=tf.float32):\n",
        "        super().__init__(name='LSTMForgetBlock', dtype=dtype)\n",
        "        self.W_f = tf.keras.layers.Dense(num_nodes, dtype=dtype, name='W_f', use_bias=False)\n",
        "        self.U_f = tf.keras.layers.Dense(num_nodes, dtype=dtype, name='U_f')\n",
        "        self.W_g = tf.keras.layers.Dense(num_nodes, dtype=dtype, name='W_g', use_bias=False)\n",
        "        self.U_g = tf.keras.layers.Dense(num_nodes, dtype=dtype, name='U_g')\n",
        "        self.W_r = tf.keras.layers.Dense(num_nodes, dtype=dtype, name='W_r', use_bias=False)\n",
        "        self.U_r = tf.keras.layers.Dense(num_nodes, dtype=dtype, name='U_r')\n",
        "        self.W_s = tf.keras.layers.Dense(num_nodes, dtype=dtype, name='W_s', use_bias=False)\n",
        "        self.U_s = tf.keras.layers.Dense(num_nodes, dtype=dtype, name='U_s')\n",
        "\n",
        "    def call(self, x, h, c):\n",
        "        f = tf.keras.activations.tanh(self.W_f(x) + self.U_f(h))\n",
        "        g = tf.keras.activations.tanh(self.W_g(x) + self.U_g(h))\n",
        "        r = tf.keras.activations.tanh(self.W_r(x) + self.U_r(h))\n",
        "        s = tf.keras.activations.tanh(self.W_s(x) + self.U_s(h))\n",
        "        c = f*c + g*s\n",
        "        return r*tf.keras.activations.tanh(c), c\n",
        "\n",
        "\n",
        "class LSTMForgetNet(tf.keras.models.Model):\n",
        "    \"\"\"\n",
        "    Description: \n",
        "        LSTM Forget architecture\n",
        "    Args:\n",
        "        num_nodes: number of nodes in each LSTM layer\n",
        "        num_layers: number of LSTM layers\n",
        "    \"\"\"\n",
        "    def __init__(self, num_nodes, num_layers, dtype=tf.float32, name = 'LSTMForgetNet'):\n",
        "        super().__init__(dtype=dtype, name=name)\n",
        "        self.num_nodes = num_nodes\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm_layers = [LSTMForgetBlock(num_nodes, dtype=dtype) for _ in range(num_layers)]\n",
        "        self.final_dense = tf.keras.layers.Dense(units=1, activation=None, dtype=dtype)\n",
        "        self.batch_norm = tf.keras.layers.BatchNormalization(axis=1)\n",
        "        \n",
        "        \n",
        "\n",
        "    def call(self, *args):\n",
        "        x = tf.concat(args, axis=1)\n",
        "        h = tf.zeros_like(x)\n",
        "        c = tf.zeros((x.shape[0], self.num_nodes), dtype=self.dtype)\n",
        "        for i in range(self.num_layers):\n",
        "            h, c = self.lstm_layers[i](x, h, c)\n",
        "            h = self.batch_norm(h)\n",
        "            c = self.batch_norm(c)\n",
        "        y = self.final_dense(h)\n",
        "        return y\n",
        "\n",
        "\n",
        "surface = LSTMForgetNet(num_nodes=50, num_layers=3)\n",
        "mu = LSTMForgetNet(num_nodes=20, num_layers=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the area functional**"
      ],
      "metadata": {
        "id": "dRTX8Df4sV1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def minimal_op(u, r, t):\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    tape.watch([r, t])\n",
        "    u_ = u(r, t)\n",
        "    u_r, u_t = tape.gradient(u_, [r, t])\n",
        "  return tf.sqrt(r**2 * (1.0 + u_r**2) + u_t**2)\n",
        "    "
      ],
      "metadata": {
        "id": "TI8vst8WxY67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the boundary condition**"
      ],
      "metadata": {
        "id": "1oZemUtE0bP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_boundary = [0., 4.0 * np.pi]\n",
        "\n",
        "def helix_boundary(u, t):\n",
        "  return u(tf.ones_like(t), t) - t"
      ],
      "metadata": {
        "id": "2GFSYOf_yIKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define domain and boundary samplers**"
      ],
      "metadata": {
        "id": "GE5YeThDVlAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def domain_sampler(n_sample, low=[0., t_boundary[0]], high=[1., t_boundary[1]]):\n",
        "  r = tf.random.uniform(shape=(n_sample, 1), minval=low[0], maxval=high[0])\n",
        "  t = tf.random.uniform(shape=(n_sample, 1), minval=low[1], maxval=high[1])\n",
        "  return r, t \n",
        "\n",
        "def boundary_sampler(n_sample, low=t_boundary[0], high=t_boundary[1]):\n",
        "  t = tf.random.uniform(shape=(n_sample, 1), minval=low, maxval=high)\n",
        "  return t"
      ],
      "metadata": {
        "id": "rfH763ahqXK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the loss function and the training procedure**"
      ],
      "metadata": {
        "id": "9sek5-VF27Ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_surface(network, mul, r, t, t_b, rho):\n",
        "  return tf.reduce_mean(minimal_op(network, r, t)) + tf.reduce_mean(helix_boundary(network, t_b) * mul(tf.ones_like(t_b), t_b)) \\\n",
        "         + 0.5 * rho * tf.reduce_mean(helix_boundary(surface, t_b)**2)\n",
        "\n",
        "def loss_mu(network, mul, mu_0, t_b, rho):\n",
        "  return tf.reduce_mean((mul(tf.ones_like(t_b), t_b) - mu_0 - rho * helix_boundary(network, t_b))**2)\n",
        "  \n",
        "\n",
        "@tf.function\n",
        "def train_step_surface(optimizer, network, mul, r, t, t_b, rho):\n",
        "  with tf.GradientTape() as tape:\n",
        "    L = loss_surface(network, mul, r, t, t_b, rho)\n",
        "  grads = tape.gradient(L, network.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, network.trainable_weights))\n",
        "  return L\n",
        "\n",
        "@tf.function\n",
        "def train_step_mu(optimizer, network, mul, t_b, rho):\n",
        "  mu_0 = mu(tf.ones_like(t_b), t_b)\n",
        "  with tf.GradientTape() as tape:\n",
        "    L = loss_mu(network, mul, mu_0, t_b, rho)\n",
        "  grads = tape.gradient(L, mul.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, mul.trainable_weights))\n",
        "  return L\n",
        "\n",
        "def learn(network, mul, rho, aleph, epochs=10000, n_sample=1000):\n",
        "  r, t = domain_sampler(n_sample)\n",
        "  t_b = boundary_sampler(n_sample)\n",
        "  learning_rate_1 = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000, 2000, 10000], [5e-3, 1e-3, 5e-4, 1e-4])\n",
        "  optimizer_1 = tf.keras.optimizers.Adam(learning_rate_1)\n",
        "  learning_rate_2 = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000, 2000, 10000], [5e-3, 1e-3, 5e-4, 1e-4])\n",
        "  optimizer_2 = tf.keras.optimizers.Adam(learning_rate_2)\n",
        "  print(\"{:>6}{:>12}{:>12}{:>18}\".format('Epoch', 'Loss_1', 'Loss_2', 'Runtime(s)'))\n",
        "  start = time.time()\n",
        "  for epoch in range(epochs):\n",
        "    L_1 = train_step_surface(optimizer_1, network, mul, r, t, t_b, rho)\n",
        "    for i in range(1):\n",
        "      L_2 = train_step_mu(optimizer_2, network, mul, t_b, rho)\n",
        "    rho *= aleph\n",
        "    if epoch % 10 == 0:\n",
        "      print('{:6d}{:12.6f}{:16.6f}{:12.4f}'.format(epoch, L_1, L_2, time.time()-start))\n",
        "      r, t = domain_sampler(n_sample)\n",
        "      t_b = boundary_sampler(n_sample)\n",
        "      surface.save_weights('surface_{}'.format(epochs))"
      ],
      "metadata": {
        "id": "pw2fwFO326Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learn the solution**"
      ],
      "metadata": {
        "id": "GwC21mxWYTkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "surface(*domain_sampler(10)) # build the network\n",
        "mu(*domain_sampler(10))\n",
        "epochs = 500\n",
        "n_sample = 1000\n",
        "rho = 50.\n",
        "aleph = 1.004\n",
        "learn(surface, mu, rho, aleph, epochs, n_sample)\n",
        "surface.save_weights('surface_{}'.format(epochs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjpymHAcx6Qr",
        "outputId": "30d452cf-0e90-40f0-a385-a62c35f8f842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch      Loss_1      Loss_2        Runtime(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualize the solution**"
      ],
      "metadata": {
        "id": "LWUwMgHuZGT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_solutions(learned, resolution=30):\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "  ax_l = fig.add_subplot(121, projection='3d')\n",
        "  ax_t = fig.add_subplot(122, projection='3d')\n",
        "  r = np.linspace(0., 1., num=resolution, endpoint=True)\n",
        "  t = np.linspace(t_boundary[0], t_boundary[1], num=resolution, endpoint=True)\n",
        "  r, t = np.meshgrid(r, t)\n",
        "  x = r * np.cos(t)\n",
        "  y = r * np.sin(t)\n",
        "  z_l = learned(r.reshape(-1, 1), t.reshape(-1, 1)).numpy()\n",
        "  z_t = t.reshape(-1, 1)\n",
        "  grid = (resolution, resolution)\n",
        "  #x = x.reshape(grid)\n",
        "  #y = y.reshape(grid)\n",
        "  z_l = z_l.reshape(grid)\n",
        "  z_t = z_t.reshape(grid)\n",
        "  ax_l.plot_surface(x, y, z_l, color='deeppink')\n",
        "  ax_l.set_title('learned minimal surface', fontsize=15)\n",
        "  ax_t.plot_surface(x, y, z_t, color='blue')\n",
        "  ax_t.set_title('helicoid', fontsize=15)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "plot_solutions(learned=lambda r, t: surface(r, t))"
      ],
      "metadata": {
        "id": "dB-sfA1yZ8YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "surface.summary()"
      ],
      "metadata": {
        "id": "K1a5jOdKqyMj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}